{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "\n",
    "# Define the data loading and preprocessing functions\n",
    "def load_data_from_h5(file_path):\n",
    "    with h5py.File(file_path, \"r\") as hf:\n",
    "        vibration_data = hf[\"vibration_data\"][:]\n",
    "    return vibration_data\n",
    "\n",
    "def normalize_data(data):\n",
    "    min_value, max_value = -2500.0, 2500.0\n",
    "    normalized_data = 2 * ((data - min_value) / (max_value - min_value)) - 1\n",
    "    return normalized_data\n",
    "\n",
    "# Load data\n",
    "good_data = load_data_from_h5(\"../data/M01/OP07/good/M01_Aug_2019_OP07_000.h5\")\n",
    "bad_data = load_data_from_h5(\"../data/M01/OP07/bad/M01_Aug_2019_OP07_000.h5\")\n",
    "\n",
    "# Normalize data\n",
    "good_data_normalized = normalize_data(good_data)\n",
    "bad_data_normalized = normalize_data(bad_data)\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_file_path = \"onnx_models/97e1a403699d408c89c536d7e4a7c09b.onnx\"\n",
    "ort_session = onnxruntime.InferenceSession(onnx_file_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "def infer_and_compute_loss(data_normalized):\n",
    "    losses = []\n",
    "    seq_length = 4000\n",
    "    num_chunks = len(data_normalized) // seq_length\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk = data_normalized[i * seq_length: (i + 1) * seq_length]\n",
    "        chunk = chunk.reshape(1, seq_length, 3).astype(np.float32)\n",
    "        ort_inputs = {input_name: chunk}\n",
    "        ort_outs = ort_session.run(None, ort_inputs)\n",
    "        reconstruction = np.array(ort_outs).squeeze()\n",
    "        loss = np.mean((reconstruction - chunk) ** 2)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "root_dir = \"../data\"\n",
    "machine_names = [\"M01\", \"M02\", \"M03\"]\n",
    "process_name = \"OP07\"\n",
    "labels = [\"good\", \"bad\"]\n",
    "\n",
    "loss_stats_machine = {machine: [] for machine in machine_names}\n",
    "loss_stats_machine_label = {machine: {label: [] for label in labels} for machine in machine_names}\n",
    "loss_stats_label = {label: [] for label in labels}\n",
    "\n",
    "for machine_name in machine_names:\n",
    "    for label in labels:\n",
    "        data_dir = os.path.join(root_dir, machine_name, process_name, label)\n",
    "        file_list = [file for file in os.listdir(data_dir) if file.endswith(\".h5\")]\n",
    "\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(data_dir, file_name)\n",
    "            data = load_data_from_h5(file_path)\n",
    "            data_normalized = normalize_data(data)\n",
    "\n",
    "            mse_loss = infer_and_compute_loss(data_normalized)\n",
    "\n",
    "            print(f\"MSE Loss for {file_name} in machine {machine_name} with label {label}: {mse_loss}\")\n",
    "\n",
    "            loss_stats_machine[machine_name].append(mse_loss)\n",
    "            loss_stats_machine_label[machine_name][label].append(mse_loss)\n",
    "            loss_stats_label[label].append(mse_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStatistics:\")\n",
    "\n",
    "print(\"\\nAverage, Min, Max Loss Per Machine:\")\n",
    "for machine, losses in loss_stats_machine.items():\n",
    "    print(f\"{machine}: Avg: {sum(losses)/len(losses):.5f}, Min: {min(losses):.5f}, Max: {max(losses):.5f}\")\n",
    "\n",
    "print(\"\\nAverage, Min, Max Loss Per Machine Per Label:\")\n",
    "for machine, label_losses in loss_stats_machine_label.items():\n",
    "    for label, losses in label_losses.items():\n",
    "        print(f\"{machine} - {label}: Avg: {sum(losses)/len(losses):.5f}, Min: {min(losses):.5f}, Max: {max(losses):.5f}\")\n",
    "\n",
    "print(\"\\nAverage, Min, Max Loss Per Label:\")\n",
    "for label, losses in loss_stats_label.items():\n",
    "    print(f\"{label}: Avg: {sum(losses)/len(losses):.5f}, Min: {min(losses):.5f}, Max: {max(losses):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the threshold for classification\n",
    "threshold = min(loss_stats_label['bad']) * 0.95\n",
    "\n",
    "# Initialize counters\n",
    "true_positive = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "# Iterate over 'good' labeled data and classify\n",
    "for loss in loss_stats_label['good']:\n",
    "    if loss > threshold:\n",
    "        false_positive += 1\n",
    "    else:\n",
    "        true_negative += 1\n",
    "\n",
    "# Iterate over 'bad' labeled data and classify\n",
    "for loss in loss_stats_label['bad']:\n",
    "    if loss > threshold:\n",
    "        true_positive += 1\n",
    "    else:\n",
    "        false_negative += 1\n",
    "\n",
    "print(f\"\\nClassification Statistics based on threshold of {threshold:.5f}:\")\n",
    "print(f\"True Positives (TP): {true_positive}\")\n",
    "print(f\"True Negatives (TN): {true_negative}\")\n",
    "print(f\"False Positives (FP): {false_positive}\")\n",
    "print(f\"False Negatives (FN): {false_negative}\")\n",
    "\n",
    "print(f\"\\nTotals:\")\n",
    "print(f\"Total 'good' labeled data: {len(loss_stats_label['good'])}\")\n",
    "print(f\"Total 'bad' labeled data: {len(loss_stats_label['bad'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
